有用链接

工具
https://tool.lu/encdec/
http://tool.oschina.net/codeformat/js
内容不错：
https://www.jianshu.com/p/1476a181fc57
https://cuiqingcai.com/6182.html
https://cuiqingcai.com/4048.html的
##使用REDIS_URL链接Redis数据库, deconde_responses=True这个参数必须要，数据会变成byte形式 完全没法用
reds = redis.Redis.from_url(REDIS_URL, db=2, decode_responses=True)
间书
https://www.jianshu.com/p/777b7712249c
https://www.jianshu.com/p/de16a1093c7e

------------------------
cookiedict={}
requests.utils.add_dict_to_cookiejar(ssrequest.cookies,cookiedict)
cookiejar是个对象,将对象转为字典
cookiedict={}
for item in response.cookies:
	cookiedict[item.name]=item.value

#-------------------------------
npm install crypto-js
要用 AES 算法加密，首先我们要引入 crypto-js ，crypto-js 是一个纯 javascript 写的加密算法类库 ，
可以非常方便地在 javascript 进行 MD5、SHA1、SHA2、SHA3、RIPEMD-160 哈希散列，
进行 AES、DES、Rabbit、RC4、Triple DES 加解密


all(iterable) not all() 
all 列表， 元组元素都不为空或0
 all([])  >>>True      # 空列表
 all(())  >>>True      # 空元组

label标签一般用于form表单内

爬虫项目
https://yq.aliyun.com/articles/644882?spm=a2c4e.11155472.0.0.55af7c40a9qtM8

进阶
http://k.sina.com.cn/article_6436010038_17f9db8360010057d3.html
cookie看
scrapy中得到cookie
cookies = response.cookies.get_dict()
https://www.jianshu.com/p/7a1b8c144d83
https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#cookies-mw
#-------------------------------
scrapy的安装
https://blog.csdn.net/android_ztz/article/details/79243521
https://www.jianshu.com/p/7a1b8c144d83
https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#cookies-mw
http://k.sina.com.cn/article_6436010038_17f9db8360010057d3.html
cookie看
scrapy中得到cookie
cookies = response.cookies.get_dict()

from scrapy.http.cookies import CookieJar
cookiejar = CookieJar()
cookiejar.extract_cookies(response, response.request)

# 有些情况可能在发起登录之前会有一些请求,会陆续的产生一些cookie,
可以在第一次请求的时候将cookiejar写入到request的meta中进行传递

scrapy.Request(url, callback=self.xxx, meta={'cookiejar': cookiejar})
# 之后每次需要传递这个cookiejar对象可以从response.meta中拿到
scrapy.Request(url, callback=self.xxx, meta={'cookiejar': response.meta['cookiejar']})
yield Request(url,callback=self.parse, cookies=self.cookie,headers=self.headers, meta=self.meta)
保持上一次的cookie
https://doc.scrapy.org/en/latest/topics/request-response.html
yield Request(url,callback=self.parse,meta={'dont_merge_cookies': True})

scrapy发起request请求并提交参数(默认请求方式是：POST)
yield scrapy.FormRequest(url,callback=self.parse,formdata={},method="GET",errback=self.errparse,meta={})
#scrapy在parse方法后
from scrapy shell import inspect_response
inspect_response(response,self)

工具urllib encode转换
https://www.cnblogs.com/caicaihong/p/5687522.html
url.parse.unquote()

#-----------
安装jieba
pip install jieba 
https://github.com/fxsjy/jieba#3
参考 ：https://blog.csdn.net/still_night/article/details/78998659；http://www.ruanyifeng.com/blog/2013/03/tf-idf.html
pkuseg 分词: 
pip install pkuseg
https://github.com/yishuihanhan/pkuseg-python

wordcloud 云词 https://github.com/amueller/word_cloud
参考：https://blog.csdn.net/cy776719526/article/details/80171790;https://blog.csdn.net/u010309756/article/details/67637930
https://blog.csdn.net/qq_25819827/article/details/78991733

1. 分词
jieba.cut 方法接受三个输入参数: 需要分词的字符串；
cut_all 参数用来控制是否采用全模式，默认Flase，一般也用flase
；HMM 参数用来控制是否使用 HMM 模型

jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。
该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细

待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8
jieba.cut 以及 `jieba.cut_for_search` 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用
jieba.lcut 以及 `jieba.lcut_for_search` 直接返回 list
jieba.Tokenizer(dictionary=DEFAULT_DICT)` 新建自定义分词器，可用于同时使用不同词典。`jieba.dt` 为默认分词器，所有全局分词相关函数都是该分词器的映射。


jb=jieba.cut(str)---> 一个可迭代对象
变成列表
jbl=[i for i in jb]
jbl=jieba.lcut(str)---> 一个列表

from collections import Counter
abc = [1,2,454,3,6,3,1,3,5,6,8,4,3,8,4,2,1,2,3,]
counts = Counter(abc)
counter.most_common（）--〉变成一个含元组的列表

有个字符串相似度检测的库，difflib
import difflib
query_str = '市公安局'
s1 = '广州市邮政局'
difflib.SequenceMatcher(None, query_str, s1).quick_ratio() 

import redis
pool=redis.ConnectionPool(host='',port=6379,db=0,password=''，decode_responses=True)
r=redis.StrictRedis(connection_pool=pool)
Redis 发布订阅
https://blog.csdn.net/selina361/article/details/79708501?utm_source=blogxgwz8
import redis
pool=redis.ConnectionPool(host='',port=6379,db=0,password=''，decode_responses=True)
r=redis.StrictRedis(connection_pool=pool)

#new_
崔庆才的github
https://github.com/Python3WebSpider/ProxyPool/tree/master/proxypool
字符串转换成字典： https://www.cnblogs.com/OnlyDreams/p/7850920.html
python时间转换：  https://www.linuxidc.com/Linux/2018-12/155919.htm

https://opencv.org/releases.html（发布）
https://www.cnblogs.com/Undo-self-blog/p/8423851.html
linux社区python和opencv
https://www.linuxidc.com/topicnews.aspx?tid=17
https://www.linuxidc.com/Linux/2015-08/121400.htm

夜神模拟器
https://blog.csdn.net/hihell/article/details/86065636


